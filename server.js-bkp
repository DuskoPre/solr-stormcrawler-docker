// server.js
// Minimal Express API to control Solr, URLFrontier seeding, and StormCrawler runs.
// Keep this file in the same project where you can execute `docker compose run --rm runner`.
//
// Usage:
//   npm i express cors axios
//   node server.js
//
// ENV (override as needed):
//   PORT=4000
//   SOLR_URL=http://127.0.0.1:8983/solr/crawler
//   URLFRONTIER_HOST=localhost
//   URLFRONTIER_PORT=7071
//   RUNNER_CMD=docker compose run --rm runner
//   JAR_PATH=target/crawler-1.0.jar
//   MAIN_CLASS=com.digitalpebble.CrawlTopology
//   LOCAL_TTL=60

const express = require("express");
const cors = require("cors");
const axios = require("axios");
const { spawn, execFileSync } = require("child_process");
const fs = require("fs");
const os = require("os");
const path = require("path");

const PORT = process.env.PORT || 4000;
const SOLR_URL =
  process.env.SOLR_URL || "http://127.0.0.1:8983/solr/crawler"; // <-- base, not the admin hash URL
const URLFRONTIER_HOST = process.env.URLFRONTIER_HOST || "localhost";
const URLFRONTIER_PORT = Number(process.env.URLFRONTIER_PORT || 7071);

// Runner settings (your provided command)
const RUNNER_CMD = process.env.RUNNER_CMD || "docker compose run --rm runner";
const JAR_PATH = process.env.JAR_PATH || "target/crawler-1.0.jar";
const MAIN_CLASS = process.env.MAIN_CLASS || "com.digitalpebble.CrawlTopology";
const LOCAL_TTL = process.env.LOCAL_TTL || "60";

// Internal state for "automatic mode" process
let crawlProc = null;
let crawlTimer = null;

const app = express();
app.use(cors());
app.use(express.json({ limit: "1mb" }));

// ---------- Helpers

function isCrawlRunning() {
  return !!(crawlProc && !crawlProc.killed);
}

function stopCrawl(reason = "stopped by API") {
  if (crawlTimer) {
    clearTimeout(crawlTimer);
    crawlTimer = null;
  }
  if (crawlProc && !crawlProc.killed) {
    try {
      crawlProc.kill("SIGINT");
    } catch {}
  }
  crawlProc = null;
  console.log(`[crawl] ${reason}`);
}

function startCrawl({ mode, crawlTimeMinutes } = {}) {
  if (isCrawlRunning()) {
    throw new Error("Crawler is already running");
  }

  // storm local target/crawler-1.0.jar com.digitalpebble.CrawlTopology --local-ttl 60
  const args = [
    "run",
    "--rm",
    "runner",
    "/apache-storm-2.7.0/bin/storm",
    "local",
    JAR_PATH,
    MAIN_CLASS,
    "--local-ttl",
    String(LOCAL_TTL),
  ];

  // We keep it simple: same command for 'automatic' and 'manual'
  // For manual, we optionally apply a time limit here on the API side.
  console.log(`[crawl] starting (${mode}) -> ${RUNNER_CMD} ${args.join(" ")}`);

  // Spawn the docker compose run; keep handle so we can stop it
  crawlProc = spawn("docker", ["compose", ...args], {
    stdio: "inherit",
    env: process.env,
  });

  crawlProc.on("exit", (code, signal) => {
    console.log(`[crawl] exited (code=${code}, signal=${signal})`);
    crawlProc = null;
    if (crawlTimer) {
      clearTimeout(crawlTimer);
      crawlTimer = null;
    }
  });

  if (mode === "manual" && crawlTimeMinutes && Number(crawlTimeMinutes) > 0) {
    crawlTimer = setTimeout(
      () => stopCrawl("manual run reached time limit"),
      Number(crawlTimeMinutes) * 60 * 1000
    );
  }
}

// create a temporary file with one URL per line
function writeTempSeeds(urls) {
  const file = path.join(os.tmpdir(), `seeds-${Date.now()}.txt`);
  fs.writeFileSync(file, urls.join("\n"), "utf8");
  return file;
}

async function ensureUrlFrontierClient() {
  // If you prefer to vendor the client jar, place it in ./tools/urlfrontier-client.jar
  const jar = path.resolve("tools/urlfrontier-client.jar");
  if (!fs.existsSync(jar)) {
    // Download 2.4 as in your example; using sync curl for brevity
    const url =
      "https://repo1.maven.org/maven2/com/github/crawler-commons/urlfrontier-client/2.4/urlfrontier-client-2.4.jar";
    console.log("[frontier] fetching client jar:", url);
    // Use curl if available; you can replace with a node HTTP download if you prefer.
    execFileSync("curl", ["-sL", "-o", jar, url], { stdio: "inherit" });
  }
  return jar;
}

function putURLsIntoFrontier(jar, seedFile, crawlID) {
  // java -jar urlfrontier-client.jar PutURLs -f seeds.txt [-c <crawlID>]
  const args = ["-jar", jar, "PutURLs", "-f", seedFile];
  if (crawlID) args.push("-c", String(crawlID));

  // The client defaults to localhost:7071. If your frontier is elsewhere,
  // you can export URLFRONTIER_HOST/PORT and add --host/--port (2.5+ client supports flags).
  // For 2.4, the defaults suffice for localhost.
  console.log("[frontier] seeding:", args.join(" "));
  execFileSync("java", args, { stdio: "inherit" });
}

// ---------- Routes

app.get("/api/health", (_req, res) =>
  res.json({ ok: true, solr: SOLR_URL, frontier: { host: URLFRONTIER_HOST, port: URLFRONTIER_PORT } })
);

// --- Solr management

app.get("/api/solr/ping", async (_req, res, next) => {
  try {
    // simple select ping
    const { data } = await axios.get(`${SOLR_URL.replace(/\/$/, "")}/select`, {
      params: { q: "*:*", rows: 0 },
    });
    res.json({ ok: true, responseHeader: data.responseHeader });
  } catch (e) {
    next(e);
  }
});

app.post("/api/solr/commit", async (_req, res, next) => {
  try {
    const { data } = await axios.post(`${SOLR_URL.replace(/\/$/, "")}/update`, { commit: {} }, {
      headers: { "Content-Type": "application/json" },
    });
    res.json(data);
  } catch (e) {
    next(e);
  }
});

app.post("/api/solr/optimize", async (_req, res, next) => {
  try {
    const { data } = await axios.post(`${SOLR_URL.replace(/\/$/, "")}/update`, { optimize: {} }, {
      headers: { "Content-Type": "application/json" },
    });
    res.json(data);
  } catch (e) {
    next(e);
  }
});

app.post("/api/solr/deleteByQuery", async (req, res, next) => {
  try {
    const { q } = req.body || {};
    if (!q) return res.status(400).json({ error: "q required" });
    const { data } = await axios.post(`${SOLR_URL.replace(/\/$/, "")}/update`, { delete: { query: q } }, {
      headers: { "Content-Type": "application/json" },
    });
    res.json(data);
  } catch (e) {
    next(e);
  }
});

// --- URLFrontier seeding

app.post("/api/frontier/seeds", async (req, res, next) => {
  try {
    const { urls, crawlID } = req.body || {};
    if (!Array.isArray(urls) || urls.length === 0)
      return res.status(400).json({ error: "urls[] required" });

    const jar = await ensureUrlFrontierClient();
    const seedFile = writeTempSeeds(urls);
    try {
      putURLsIntoFrontier(jar, seedFile, crawlID);
    } finally {
      fs.unlink(seedFile, () => {});
    }
    res.json({ ok: true, count: urls.length, crawlID: crawlID || null });
  } catch (e) {
    next(e);
  }
});

// --- Crawl control (automatic/manual)

app.get("/api/crawl/status", (_req, res) => {
  res.json({
    running: isCrawlRunning(),
    mode: isCrawlRunning() ? "automatic-or-manual" : "stopped",
  });
});

app.post("/api/crawl/start", (req, res, next) => {
  try {
    const { mode = "automatic", crawlTimeMinutes } = req.body || {};
    // Depth is not directly wired in your Java topology; it can be added there if needed.
    startCrawl({ mode, crawlTimeMinutes });
    res.json({ ok: true, mode, crawlTimeMinutes: crawlTimeMinutes || null });
  } catch (e) {
    next(e);
  }
});

app.post("/api/crawl/stop", (_req, res) => {
  stopCrawl();
  res.json({ ok: true });
});

// --- Error handler
app.use((err, _req, res, _next) => {
  console.error(err);
  res.status(500).json({ error: err.message || String(err) });
});

app.listen(PORT, () => {
  console.log(`API listening on :${PORT}`);
});
